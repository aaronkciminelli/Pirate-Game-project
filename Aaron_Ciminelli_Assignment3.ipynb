{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 01:38:15.144980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-19 01:38:15.360838: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-19 01:38:15.360883: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-19 01:38:18.006978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 01:38:18.007101: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 01:38:18.007111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 01:38:24.596424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-19 01:38:24.596458: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-19 01:38:24.596478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-285bc0): /proc/driver/nvidia/version does not exist\n",
      "2023-03-19 01:38:24.618053: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-19 01:38:26.232358: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 491520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 56s 172ms/step - loss: 1.9709 - accuracy: 0.2686 - val_loss: 1.6526 - val_accuracy: 0.3923\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.6384 - accuracy: 0.3988 - val_loss: 1.4720 - val_accuracy: 0.4533\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.4864 - accuracy: 0.4590 - val_loss: 1.3824 - val_accuracy: 0.4871\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.3808 - accuracy: 0.5009 - val_loss: 1.2850 - val_accuracy: 0.5264\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 1.2997 - accuracy: 0.5329 - val_loss: 1.3290 - val_accuracy: 0.5116\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 1.2337 - accuracy: 0.5588 - val_loss: 1.3511 - val_accuracy: 0.5337\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.1798 - accuracy: 0.5787 - val_loss: 1.1031 - val_accuracy: 0.6071\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.1291 - accuracy: 0.5954 - val_loss: 1.1100 - val_accuracy: 0.5986\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.0960 - accuracy: 0.6120 - val_loss: 0.9954 - val_accuracy: 0.6452\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 1.0607 - accuracy: 0.6266 - val_loss: 1.0589 - val_accuracy: 0.6286\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 1.0307 - accuracy: 0.6341 - val_loss: 0.9435 - val_accuracy: 0.6658\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.0037 - accuracy: 0.6449 - val_loss: 0.9481 - val_accuracy: 0.6598\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.9789 - accuracy: 0.6553 - val_loss: 0.9623 - val_accuracy: 0.6595\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.9615 - accuracy: 0.6607 - val_loss: 0.9057 - val_accuracy: 0.6802\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.9431 - accuracy: 0.6686 - val_loss: 0.9333 - val_accuracy: 0.6727\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.9192 - accuracy: 0.6770 - val_loss: 0.8901 - val_accuracy: 0.6923\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.9054 - accuracy: 0.6800 - val_loss: 0.8899 - val_accuracy: 0.6829\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 0.8937 - accuracy: 0.6856 - val_loss: 0.9056 - val_accuracy: 0.6822\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 0.8796 - accuracy: 0.6909 - val_loss: 0.8766 - val_accuracy: 0.6923\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.8705 - accuracy: 0.6927 - val_loss: 0.8563 - val_accuracy: 0.7038\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 0.8810 - accuracy: 0.6956\n",
      "Test score: 0.8809778094291687\n",
      "Test accuracy: 0.6955999732017517\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# train\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 02:03:06.355647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-19 02:03:08.240696: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-19 02:03:08.240731: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-19 02:03:12.982139: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 02:03:12.982282: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 02:03:12.982292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training set images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 02:03:20.820982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-19 02:03:20.821031: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-19 02:03:20.821057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-285bc0): /proc/driver/nvidia/version does not exist\n",
      "2023-03-19 02:03:20.821295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_23438/866786514.py:64: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 3.3081 - accuracy: 0.1535\n",
      "Epoch 2/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 2.0446 - accuracy: 0.2438\n",
      "Epoch 3/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.9302 - accuracy: 0.2911\n",
      "Epoch 4/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.8579 - accuracy: 0.3171\n",
      "Epoch 5/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.8240 - accuracy: 0.3271\n",
      "Epoch 6/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.8050 - accuracy: 0.3354\n",
      "Epoch 7/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.8008 - accuracy: 0.3372\n",
      "Epoch 8/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7928 - accuracy: 0.3398\n",
      "Epoch 9/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7918 - accuracy: 0.3454\n",
      "Epoch 10/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7778 - accuracy: 0.3485\n",
      "Epoch 11/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7822 - accuracy: 0.3482\n",
      "Epoch 12/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7672 - accuracy: 0.3542\n",
      "Epoch 13/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7748 - accuracy: 0.3519\n",
      "Epoch 14/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7598 - accuracy: 0.3572\n",
      "Epoch 15/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7639 - accuracy: 0.3561\n",
      "Epoch 16/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7572 - accuracy: 0.3617\n",
      "Epoch 17/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7627 - accuracy: 0.3584\n",
      "Epoch 18/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7611 - accuracy: 0.3608\n",
      "Epoch 19/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7697 - accuracy: 0.3552\n",
      "Epoch 20/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7632 - accuracy: 0.3543\n",
      "Epoch 21/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7515 - accuracy: 0.3612\n",
      "Epoch 22/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7596 - accuracy: 0.3588\n",
      "Epoch 23/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7604 - accuracy: 0.3588\n",
      "Epoch 24/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7620 - accuracy: 0.3591\n",
      "Epoch 25/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7627 - accuracy: 0.3580\n",
      "Epoch 26/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7574 - accuracy: 0.3578\n",
      "Epoch 27/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7607 - accuracy: 0.3590\n",
      "Epoch 28/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7651 - accuracy: 0.3614\n",
      "Epoch 29/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7604 - accuracy: 0.3552\n",
      "Epoch 30/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7583 - accuracy: 0.3591\n",
      "Epoch 31/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7610 - accuracy: 0.3573\n",
      "Epoch 32/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7553 - accuracy: 0.3587\n",
      "Epoch 33/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7496 - accuracy: 0.3633\n",
      "Epoch 34/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7632 - accuracy: 0.3591\n",
      "Epoch 35/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7673 - accuracy: 0.3558\n",
      "Epoch 36/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7651 - accuracy: 0.3576\n",
      "Epoch 37/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7542 - accuracy: 0.3616\n",
      "Epoch 38/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7686 - accuracy: 0.3572\n",
      "Epoch 39/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7597 - accuracy: 0.3615\n",
      "Epoch 40/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7606 - accuracy: 0.3575\n",
      "Epoch 41/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7681 - accuracy: 0.3569\n",
      "Epoch 42/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7674 - accuracy: 0.3583\n",
      "Epoch 43/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7628 - accuracy: 0.3569\n",
      "Epoch 44/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7598 - accuracy: 0.3596\n",
      "Epoch 45/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7670 - accuracy: 0.3583\n",
      "Epoch 46/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7601 - accuracy: 0.3606\n",
      "Epoch 47/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7580 - accuracy: 0.3578\n",
      "Epoch 48/50\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.7700 - accuracy: 0.3564\n",
      "Epoch 49/50\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 1.7674 - accuracy: 0.3548\n",
      "Epoch 50/50\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 1.7651 - accuracy: 0.3588\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5651 - accuracy: 0.4263\n",
      "Test score: 1.5650702714920044\n",
      "Test accuracy: 0.4262999892234802\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Variables\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCH = 50\n",
    "VERBOSE = 1\n",
    "\n",
    "# Create preview directory\n",
    "if not os.path.exists('preview'):\n",
    "    os.makedirs('preview')\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Augment training set images\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the data generator\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                              steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "                              epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Ethical and Privacy Implications of Image Classification Algorithms\n",
    "\n",
    "While developing image classification algorithms like the one created in this exercise has significant potential for positive impact, it raises important ethical and privacy concerns.\n",
    "\n",
    "One primary concern is the potential for such algorithms to be used for facial recognition. The ability to accurately identify individuals from images has already been developed. Law enforcement agencies use it for purposes such as identifying suspects and tracking criminal activity. However, the use of facial recognition technology has been criticized for its potential to infringe on privacy rights and for its potential to perpetuate racial biases.\n",
    "\n",
    "### Facial Recognition and Privacy\n",
    "\n",
    "Facial recognition technology can be used to identify individuals without their knowledge or consent. However, this poses a significant risk to personal privacy, as it enables large-scale surveillance and tracking of individuals in public spaces. For example, governments and law enforcement agencies could use this technology to monitor political activists, journalists, or minority groups (Levinson-Waldman & Díaz, How to reform police monitoring of social media 2020).\n",
    "\n",
    "According to research by Joy Buolamwini, P.h.D., facial recognition technology has also been less accurate for people of color, women, and children (Dhinakaran, Coded bias: An insightful look at ai, algorithms and their risks to society 2023). In addition, in some states and many countries that use facial recognition technology, individuals are constantly monitored, and their movements are tracked without their knowledge or consent. This has resulted in misidentifications and false arrests.\n",
    "\n",
    "Additionally, the widespread use of facial recognition technology could lead to a surveillance state, with citizens constantly monitored and tracked by the government. This would severely limit people's freedom and privacy, as any action could be monitored and used against them.\n",
    "\n",
    "### Potential for Discrimination and Bias\n",
    "\n",
    "Another ethical issue with image classification algorithms is the potential for biased training data. Algorithms are only as good as the data they are trained on, and if the training data is biased, the algorithm will be as well. For example, large datasets may not represent human populations' diversity. An algorithm trained on images primarily featuring white people may be less accurate at identifying people of color. This can lead to disparities in the accuracy of facial recognition systems, exacerbate existing social inequalities, and lead to or justify discrimination.\n",
    "\n",
    "\n",
    "### Regulation and Oversight\n",
    "\n",
    "To ensure that image classification algorithms are developed and used ethically, it is essential to consider the potential implications carefully and implement safeguards to mitigate risks. This could include rigorous testing to ensure accuracy and fairness, and transparency in the development and use of algorithms. For example, an algorithm could be tested to ensure it performs equally well on images of people with different genders, skin tones, and age groups. This would avoid bias or prejudice in the results.\n",
    "\n",
    "Companies should also have processes to regularly review their algorithms and ensure they remain unbiased. Accountability measures should be established for algorithm developers and users to ensure any potential biases are addressed. Additionally, companies should be open to feedback from stakeholders and the public to ensure that any issues with algorithms are identified and managed in a timely manner.\n",
    "In conclusion, while the current algorithm is focused on classifying images of animals and vehicles, its potential extension to facial recognition raises important ethical and privacy concerns. Therefore, as advanced AI technologies continue to develop and deploy, it is crucial to consider their broader societal implications and establish mechanisms to ensure their responsible use.\n",
    "\n",
    "## References\n",
    "\n",
    "Levinson-Waldman, R., & Díaz, Á. (2020, July 20). How to reform police monitoring of social media. Brookings. Retrieved March 20, 2023, from https://www.brookings.edu/techstream/how-to-reform-police-monitoring-of-social-media/ \n",
    "\n",
    "Dhinakaran, A. (2023, February 24). Coded bias: An insightful look at ai, algorithms and their risks to society. Forbes. Retrieved March 20, 2023, from https://www.forbes.com/sites/aparnadhinakaran/2021/04/15/coded-bias-an-insightful-look-at-ai-algorithms-and-their-risks-to-society/?sh=5c46425e2fd6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
