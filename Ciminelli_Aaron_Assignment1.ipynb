{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 1.7510 - accuracy: 0.4483 - val_loss: 0.9376 - val_accuracy: 0.8153\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.9322 - accuracy: 0.7170 - val_loss: 0.5394 - val_accuracy: 0.8643\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.7011 - accuracy: 0.7854 - val_loss: 0.4318 - val_accuracy: 0.8860\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.5971 - accuracy: 0.8186 - val_loss: 0.3793 - val_accuracy: 0.8969\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.5332 - accuracy: 0.8413 - val_loss: 0.3445 - val_accuracy: 0.9043\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.4941 - accuracy: 0.8519 - val_loss: 0.3200 - val_accuracy: 0.9105\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.4597 - accuracy: 0.8645 - val_loss: 0.3006 - val_accuracy: 0.9143\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.4299 - accuracy: 0.8710 - val_loss: 0.2840 - val_accuracy: 0.9204\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.4146 - accuracy: 0.8768 - val_loss: 0.2724 - val_accuracy: 0.9229\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.3929 - accuracy: 0.8842 - val_loss: 0.2599 - val_accuracy: 0.9258\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.3785 - accuracy: 0.8882 - val_loss: 0.2494 - val_accuracy: 0.9287\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.3585 - accuracy: 0.8943 - val_loss: 0.2395 - val_accuracy: 0.9312\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.3500 - accuracy: 0.8969 - val_loss: 0.2324 - val_accuracy: 0.9327\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.3362 - accuracy: 0.9024 - val_loss: 0.2247 - val_accuracy: 0.9352\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.3287 - accuracy: 0.9041 - val_loss: 0.2177 - val_accuracy: 0.9366\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.3196 - accuracy: 0.9056 - val_loss: 0.2113 - val_accuracy: 0.9383\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.3072 - accuracy: 0.9085 - val_loss: 0.2056 - val_accuracy: 0.9402\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.2997 - accuracy: 0.9115 - val_loss: 0.2000 - val_accuracy: 0.9420\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.2929 - accuracy: 0.9137 - val_loss: 0.1948 - val_accuracy: 0.9428\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.2852 - accuracy: 0.9162 - val_loss: 0.1900 - val_accuracy: 0.9448\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Test score: 0.1915933752641082\n",
      "Test accuracy: 0.9433000087738037\n"
     ]
    }
   ],
   "source": [
    "# from Deep learing with Keras Pages 25-26\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 785       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 805\n",
      "Trainable params: 805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 2.2631 - accuracy: 0.1520 - val_loss: 2.2156 - val_accuracy: 0.1788\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 2.1705 - accuracy: 0.1904 - val_loss: 2.1191 - val_accuracy: 0.1926\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 2.0827 - accuracy: 0.2016 - val_loss: 2.0390 - val_accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 2.0127 - accuracy: 0.2088 - val_loss: 1.9791 - val_accuracy: 0.2070\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 1.9625 - accuracy: 0.2148 - val_loss: 1.9370 - val_accuracy: 0.2153\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 1.9268 - accuracy: 0.2233 - val_loss: 1.9066 - val_accuracy: 0.2180\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 1.9002 - accuracy: 0.2226 - val_loss: 1.8836 - val_accuracy: 0.2344\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 1.8796 - accuracy: 0.2298 - val_loss: 1.8651 - val_accuracy: 0.2220\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 1.8632 - accuracy: 0.2312 - val_loss: 1.8507 - val_accuracy: 0.2183\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 1.8497 - accuracy: 0.2310 - val_loss: 1.8386 - val_accuracy: 0.2285\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 1.8385 - accuracy: 0.2338 - val_loss: 1.8282 - val_accuracy: 0.2240\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 1.8288 - accuracy: 0.2335 - val_loss: 1.8197 - val_accuracy: 0.2350\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 1.8205 - accuracy: 0.2376 - val_loss: 1.8119 - val_accuracy: 0.2286\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 1.8130 - accuracy: 0.2359 - val_loss: 1.8051 - val_accuracy: 0.2404\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 1.8064 - accuracy: 0.2379 - val_loss: 1.7991 - val_accuracy: 0.2362\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 1.8003 - accuracy: 0.2386 - val_loss: 1.7935 - val_accuracy: 0.2335\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 1.7948 - accuracy: 0.2400 - val_loss: 1.7880 - val_accuracy: 0.2371\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 1.7897 - accuracy: 0.2407 - val_loss: 1.7837 - val_accuracy: 0.2390\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 1.7848 - accuracy: 0.2466 - val_loss: 1.7784 - val_accuracy: 0.2343\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 1.7804 - accuracy: 0.2443 - val_loss: 1.7743 - val_accuracy: 0.2388\n",
      "10000/10000 [==============================] - 0s 30us/step\n",
      "Number of Hidden Layers: 1\n",
      "Test score: 1.778583079147339\n",
      "Test accuracy: 0.24070000648498535\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 5)                 3925      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,105\n",
      "Trainable params: 4,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 2.2985 - accuracy: 0.1358 - val_loss: 2.2871 - val_accuracy: 0.1573\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 2.2257 - accuracy: 0.1657 - val_loss: 2.1373 - val_accuracy: 0.1862\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 2.0415 - accuracy: 0.2150 - val_loss: 1.9606 - val_accuracy: 0.2495\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 1.9099 - accuracy: 0.2912 - val_loss: 1.8482 - val_accuracy: 0.3473\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 1.7563 - accuracy: 0.3889 - val_loss: 1.6381 - val_accuracy: 0.4437\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 1.5526 - accuracy: 0.4538 - val_loss: 1.4788 - val_accuracy: 0.4350\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 1.4051 - accuracy: 0.4733 - val_loss: 1.3244 - val_accuracy: 0.5354\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 1.2622 - accuracy: 0.5701 - val_loss: 1.1977 - val_accuracy: 0.5949\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 1.1550 - accuracy: 0.6109 - val_loss: 1.1065 - val_accuracy: 0.6242\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 1.0748 - accuracy: 0.6418 - val_loss: 1.0280 - val_accuracy: 0.6603\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 1.0065 - accuracy: 0.6749 - val_loss: 0.9611 - val_accuracy: 0.6894\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.9401 - accuracy: 0.7065 - val_loss: 0.9073 - val_accuracy: 0.7137\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.8811 - accuracy: 0.7313 - val_loss: 0.8419 - val_accuracy: 0.7487\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.8271 - accuracy: 0.7603 - val_loss: 0.7872 - val_accuracy: 0.7736\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.7769 - accuracy: 0.7819 - val_loss: 0.7422 - val_accuracy: 0.7920\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.7371 - accuracy: 0.7946 - val_loss: 0.7127 - val_accuracy: 0.8010\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.7039 - accuracy: 0.8051 - val_loss: 0.6781 - val_accuracy: 0.8140\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.6748 - accuracy: 0.8152 - val_loss: 0.6503 - val_accuracy: 0.8173\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.6488 - accuracy: 0.8224 - val_loss: 0.6378 - val_accuracy: 0.8256\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.6272 - accuracy: 0.8281 - val_loss: 0.6046 - val_accuracy: 0.8317\n",
      "10000/10000 [==============================] - 0s 18us/step\n",
      "Number of Hidden Layers: 5\n",
      "Test score: 0.6047949760913849\n",
      "Test accuracy: 0.8364999890327454\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 23,890\n",
      "Trainable params: 23,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 2.3020 - accuracy: 0.1136 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.3012 - val_accuracy: 0.1060\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 2.3002 - accuracy: 0.1140 - val_loss: 2.3003 - val_accuracy: 0.1060\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 2.2989 - accuracy: 0.1140 - val_loss: 2.2985 - val_accuracy: 0.1060\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 2.2948 - accuracy: 0.1140 - val_loss: 2.2892 - val_accuracy: 0.1060\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 2.2744 - accuracy: 0.1743 - val_loss: 2.2510 - val_accuracy: 0.2076\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 2.1960 - accuracy: 0.2110 - val_loss: 2.1092 - val_accuracy: 0.1924\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 3s 61us/step - loss: 2.0148 - accuracy: 0.2145 - val_loss: 1.9291 - val_accuracy: 0.2123\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 1.8884 - accuracy: 0.2188 - val_loss: 1.8396 - val_accuracy: 0.2176\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.8153 - accuracy: 0.2255 - val_loss: 1.8286 - val_accuracy: 0.2321\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 1.7641 - accuracy: 0.2403 - val_loss: 1.7313 - val_accuracy: 0.2589\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.7210 - accuracy: 0.2742 - val_loss: 1.8199 - val_accuracy: 0.2227\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.6315 - accuracy: 0.3192 - val_loss: 1.4896 - val_accuracy: 0.3833\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 1.4820 - accuracy: 0.3809 - val_loss: 1.3530 - val_accuracy: 0.4400\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 1.3125 - accuracy: 0.4547 - val_loss: 1.2244 - val_accuracy: 0.5198\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 1.1827 - accuracy: 0.5110 - val_loss: 1.1533 - val_accuracy: 0.5542\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 1.1273 - accuracy: 0.5390 - val_loss: 1.0705 - val_accuracy: 0.5749\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 1.0810 - accuracy: 0.5716 - val_loss: 1.0402 - val_accuracy: 0.6302\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 1.0275 - accuracy: 0.6063 - val_loss: 0.9796 - val_accuracy: 0.6347\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.9737 - accuracy: 0.6379 - val_loss: 0.9557 - val_accuracy: 0.6442\n",
      "10000/10000 [==============================] - 0s 26us/step\n",
      "Number of Hidden Layers: 20\n",
      "Test score: 0.9610460734367371\n",
      "Test accuracy: 0.6309999823570251\n"
     ]
    }
   ],
   "source": [
    "# Aaron Cimielli - March 11 2023 \n",
    "# Hidden Layers Experiment\n",
    "# For this experiment I modified the Number of hidden layers\n",
    "# N_HIDDEN [1, 5, 20] \n",
    "# After each model we printed I printed the number of hidden layers and the test accuracy \n",
    "# \n",
    "\n",
    "# Examples were found in chapter 1 and 2 from Deep learing with Keras\n",
    "# and https://keras.io\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = [1, 5, 20]\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "for i in range(len(N_HIDDEN)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(N_HIDDEN[i], input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    for j in range(N_HIDDEN[i]-1):\n",
    "        model.add(Dense(N_HIDDEN[i]))\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(NB_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train,\n",
    "    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "    score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "    print(f\"Number of Hidden Layers: {N_HIDDEN[i]}\")\n",
    "    print(\"Test score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Experiment with batch size of 32\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 169us/step - loss: 0.7164 - accuracy: 0.8124 - val_loss: 0.3420 - val_accuracy: 0.9043\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 8s 168us/step - loss: 0.3273 - accuracy: 0.9062 - val_loss: 0.2778 - val_accuracy: 0.9203\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.2748 - accuracy: 0.9214 - val_loss: 0.2444 - val_accuracy: 0.9300\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 8s 173us/step - loss: 0.2403 - accuracy: 0.9320 - val_loss: 0.2195 - val_accuracy: 0.9381\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 9s 181us/step - loss: 0.2139 - accuracy: 0.9390 - val_loss: 0.1988 - val_accuracy: 0.9446\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.1929 - accuracy: 0.9452 - val_loss: 0.1847 - val_accuracy: 0.9464\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.1748 - accuracy: 0.9505 - val_loss: 0.1733 - val_accuracy: 0.9507\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.1598 - accuracy: 0.9546 - val_loss: 0.1625 - val_accuracy: 0.9546\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 9s 181us/step - loss: 0.1473 - accuracy: 0.9585 - val_loss: 0.1508 - val_accuracy: 0.9573\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 9s 186us/step - loss: 0.1364 - accuracy: 0.9616 - val_loss: 0.1452 - val_accuracy: 0.9595\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 10s 199us/step - loss: 0.1264 - accuracy: 0.9644 - val_loss: 0.1402 - val_accuracy: 0.9596\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 9s 184us/step - loss: 0.1176 - accuracy: 0.9669 - val_loss: 0.1338 - val_accuracy: 0.9612\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 10s 204us/step - loss: 0.1102 - accuracy: 0.9687 - val_loss: 0.1367 - val_accuracy: 0.9611\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 9s 186us/step - loss: 0.1035 - accuracy: 0.9707 - val_loss: 0.1278 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.0969 - accuracy: 0.9729 - val_loss: 0.1203 - val_accuracy: 0.9645\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 9s 190us/step - loss: 0.0915 - accuracy: 0.9743 - val_loss: 0.1162 - val_accuracy: 0.9663\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 9s 184us/step - loss: 0.0861 - accuracy: 0.9759 - val_loss: 0.1122 - val_accuracy: 0.9678\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.0813 - accuracy: 0.9775 - val_loss: 0.1106 - val_accuracy: 0.9675\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 8s 176us/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 0.1096 - val_accuracy: 0.9681\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.0731 - accuracy: 0.9799 - val_loss: 0.1065 - val_accuracy: 0.9686\n",
      "Test score: 0.09733273428641259\n",
      "Test accuracy: 0.9706000089645386\n",
      "\n",
      "Experiment with batch size of 64\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.0669 - accuracy: 0.9820 - val_loss: 0.1064 - val_accuracy: 0.9682\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.0653 - accuracy: 0.9823 - val_loss: 0.1023 - val_accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.0635 - accuracy: 0.9828 - val_loss: 0.1017 - val_accuracy: 0.9689\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0617 - accuracy: 0.9835 - val_loss: 0.1019 - val_accuracy: 0.9691\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.0602 - accuracy: 0.9840 - val_loss: 0.1028 - val_accuracy: 0.9693\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.0586 - accuracy: 0.9841 - val_loss: 0.0994 - val_accuracy: 0.9703\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0570 - accuracy: 0.9847 - val_loss: 0.1003 - val_accuracy: 0.9705\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.0555 - accuracy: 0.9853 - val_loss: 0.0983 - val_accuracy: 0.9707\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.0543 - accuracy: 0.9855 - val_loss: 0.0993 - val_accuracy: 0.9699\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0530 - accuracy: 0.9857 - val_loss: 0.0970 - val_accuracy: 0.9709\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0517 - accuracy: 0.9865 - val_loss: 0.0969 - val_accuracy: 0.9708\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.0502 - accuracy: 0.9869 - val_loss: 0.0958 - val_accuracy: 0.9713\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.0488 - accuracy: 0.9872 - val_loss: 0.0967 - val_accuracy: 0.9703\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.0478 - accuracy: 0.9877 - val_loss: 0.0951 - val_accuracy: 0.9710\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.0466 - accuracy: 0.9879 - val_loss: 0.0953 - val_accuracy: 0.9716\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0454 - accuracy: 0.9882 - val_loss: 0.0951 - val_accuracy: 0.9712\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.0444 - accuracy: 0.9886 - val_loss: 0.0954 - val_accuracy: 0.9719\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.0435 - accuracy: 0.9891 - val_loss: 0.0945 - val_accuracy: 0.9720\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.0422 - accuracy: 0.9892 - val_loss: 0.0938 - val_accuracy: 0.9728\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.0411 - accuracy: 0.9896 - val_loss: 0.0934 - val_accuracy: 0.9726\n",
      "Test score: 0.08398082950138487\n",
      "Test accuracy: 0.9739999771118164\n",
      "\n",
      "Experiment with batch size of 256\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0388 - accuracy: 0.9906 - val_loss: 0.0924 - val_accuracy: 0.9721\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0385 - accuracy: 0.9907 - val_loss: 0.0925 - val_accuracy: 0.9728\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0382 - accuracy: 0.9908 - val_loss: 0.0923 - val_accuracy: 0.9726\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0380 - accuracy: 0.9910 - val_loss: 0.0919 - val_accuracy: 0.9723\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0377 - accuracy: 0.9909 - val_loss: 0.0921 - val_accuracy: 0.9725\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0375 - accuracy: 0.9911 - val_loss: 0.0917 - val_accuracy: 0.9732\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0373 - accuracy: 0.9909 - val_loss: 0.0923 - val_accuracy: 0.9725\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0370 - accuracy: 0.9910 - val_loss: 0.0920 - val_accuracy: 0.9725\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0367 - accuracy: 0.9913 - val_loss: 0.0920 - val_accuracy: 0.9724\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0366 - accuracy: 0.9915 - val_loss: 0.0919 - val_accuracy: 0.9728\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0363 - accuracy: 0.9914 - val_loss: 0.0916 - val_accuracy: 0.9727\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.0361 - accuracy: 0.9915 - val_loss: 0.0916 - val_accuracy: 0.9731\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0359 - accuracy: 0.9916 - val_loss: 0.0915 - val_accuracy: 0.9722\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0357 - accuracy: 0.9914 - val_loss: 0.0922 - val_accuracy: 0.9728\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0355 - accuracy: 0.9917 - val_loss: 0.0915 - val_accuracy: 0.9730\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0353 - accuracy: 0.9916 - val_loss: 0.0914 - val_accuracy: 0.9731\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0350 - accuracy: 0.9919 - val_loss: 0.0913 - val_accuracy: 0.9728\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0349 - accuracy: 0.9919 - val_loss: 0.0913 - val_accuracy: 0.9732\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0347 - accuracy: 0.9919 - val_loss: 0.0915 - val_accuracy: 0.9725\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0345 - accuracy: 0.9921 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Test score: 0.08098883743807674\n",
      "Test accuracy: 0.9750999808311462\n"
     ]
    }
   ],
   "source": [
    "# Aaron Cimielli - March 11 2023 \n",
    "# Batch Size \n",
    "# for this experiment I modified the batch size.\n",
    "# Specifically, I decided to use 3 different values, \"32, 64, and 256\". \n",
    "# Here I am using two hidden layers. \n",
    "\n",
    "# Examples were found in chapter 1 and 2 from Deep learing with Keras\n",
    "# and from https://keras.io/api/models/model_training_apis ( site found in chapter 1 )\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# 2 hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Experiment 2: Change batch size\n",
    "batch_sizes = [32, 64, 256]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nExperiment with batch size of {batch_size}\")\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        batch_size=batch_size, epochs=NB_EPOCH,\n",
    "                        verbose=1, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Experiment with 10 epochs\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 1.4277 - accuracy: 0.6402 - val_loss: 0.7195 - val_accuracy: 0.8405\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.5871 - accuracy: 0.8504 - val_loss: 0.4510 - val_accuracy: 0.8830\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.4395 - accuracy: 0.8789 - val_loss: 0.3756 - val_accuracy: 0.8947\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.3820 - accuracy: 0.8919 - val_loss: 0.3403 - val_accuracy: 0.9031\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3492 - accuracy: 0.9003 - val_loss: 0.3155 - val_accuracy: 0.9096\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.3267 - accuracy: 0.9065 - val_loss: 0.2979 - val_accuracy: 0.9146\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3090 - accuracy: 0.9121 - val_loss: 0.2836 - val_accuracy: 0.9199\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2944 - accuracy: 0.9159 - val_loss: 0.2711 - val_accuracy: 0.9232\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2821 - accuracy: 0.9200 - val_loss: 0.2614 - val_accuracy: 0.9266\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.2710 - accuracy: 0.9228 - val_loss: 0.2534 - val_accuracy: 0.9284\n",
      "Test score: 0.25287216559052467\n",
      "Test accuracy: 0.9276999831199646\n",
      "\n",
      "Experiment with 30 epochs\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2609 - accuracy: 0.9259 - val_loss: 0.2441 - val_accuracy: 0.9314\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2514 - accuracy: 0.9282 - val_loss: 0.2354 - val_accuracy: 0.9338\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2426 - accuracy: 0.9312 - val_loss: 0.2303 - val_accuracy: 0.9356\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2345 - accuracy: 0.9338 - val_loss: 0.2220 - val_accuracy: 0.9398\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2269 - accuracy: 0.9359 - val_loss: 0.2157 - val_accuracy: 0.9404\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2197 - accuracy: 0.9376 - val_loss: 0.2110 - val_accuracy: 0.9413\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2130 - accuracy: 0.9394 - val_loss: 0.2053 - val_accuracy: 0.9424\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2066 - accuracy: 0.9419 - val_loss: 0.1993 - val_accuracy: 0.9453\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2003 - accuracy: 0.9432 - val_loss: 0.1946 - val_accuracy: 0.9466\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.1947 - accuracy: 0.9446 - val_loss: 0.1902 - val_accuracy: 0.9477\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.1894 - accuracy: 0.9463 - val_loss: 0.1858 - val_accuracy: 0.9497\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1841 - accuracy: 0.9478 - val_loss: 0.1812 - val_accuracy: 0.9507\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1793 - accuracy: 0.9491 - val_loss: 0.1781 - val_accuracy: 0.9511\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1748 - accuracy: 0.9499 - val_loss: 0.1741 - val_accuracy: 0.9528\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.1705 - accuracy: 0.9516 - val_loss: 0.1708 - val_accuracy: 0.9541\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.1664 - accuracy: 0.9527 - val_loss: 0.1675 - val_accuracy: 0.9542\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.1621 - accuracy: 0.9543 - val_loss: 0.1645 - val_accuracy: 0.9556\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1585 - accuracy: 0.9550 - val_loss: 0.1627 - val_accuracy: 0.9563\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1549 - accuracy: 0.9563 - val_loss: 0.1602 - val_accuracy: 0.9566\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1513 - accuracy: 0.9573 - val_loss: 0.1571 - val_accuracy: 0.9569\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.1482 - accuracy: 0.9580 - val_loss: 0.1543 - val_accuracy: 0.9581\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.1449 - accuracy: 0.9594 - val_loss: 0.1532 - val_accuracy: 0.9585\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1419 - accuracy: 0.9603 - val_loss: 0.1499 - val_accuracy: 0.9590\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1388 - accuracy: 0.9612 - val_loss: 0.1481 - val_accuracy: 0.9606\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.1359 - accuracy: 0.9620 - val_loss: 0.1473 - val_accuracy: 0.9601\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.1333 - accuracy: 0.9629 - val_loss: 0.1441 - val_accuracy: 0.9605\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1306 - accuracy: 0.9634 - val_loss: 0.1426 - val_accuracy: 0.9628\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1278 - accuracy: 0.9639 - val_loss: 0.1413 - val_accuracy: 0.9617\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1256 - accuracy: 0.9650 - val_loss: 0.1401 - val_accuracy: 0.9632\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1232 - accuracy: 0.9655 - val_loss: 0.1378 - val_accuracy: 0.9635\n",
      "Test score: 0.13116495513021945\n",
      "Test accuracy: 0.9613000154495239\n",
      "\n",
      "Experiment with 50 epochs\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1207 - accuracy: 0.9661 - val_loss: 0.1363 - val_accuracy: 0.9626\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1186 - accuracy: 0.9667 - val_loss: 0.1350 - val_accuracy: 0.9635\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1165 - accuracy: 0.9674 - val_loss: 0.1333 - val_accuracy: 0.9638\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1143 - accuracy: 0.9681 - val_loss: 0.1315 - val_accuracy: 0.9645\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 9s 194us/step - loss: 0.1121 - accuracy: 0.9684 - val_loss: 0.1308 - val_accuracy: 0.9653\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.1103 - accuracy: 0.9688 - val_loss: 0.1290 - val_accuracy: 0.9653\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 12s 248us/step - loss: 0.1083 - accuracy: 0.9697 - val_loss: 0.1285 - val_accuracy: 0.9650\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 23s 469us/step - loss: 0.1064 - accuracy: 0.9699 - val_loss: 0.1268 - val_accuracy: 0.9661\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 16s 334us/step - loss: 0.1044 - accuracy: 0.9707 - val_loss: 0.1270 - val_accuracy: 0.9648\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 17s 346us/step - loss: 0.1029 - accuracy: 0.9714 - val_loss: 0.1245 - val_accuracy: 0.9658\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 14s 281us/step - loss: 0.1011 - accuracy: 0.9718 - val_loss: 0.1231 - val_accuracy: 0.9657\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 17s 347us/step - loss: 0.0994 - accuracy: 0.9721 - val_loss: 0.1224 - val_accuracy: 0.9668\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 14s 284us/step - loss: 0.0978 - accuracy: 0.9726 - val_loss: 0.1212 - val_accuracy: 0.9673\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 11s 223us/step - loss: 0.0962 - accuracy: 0.9732 - val_loss: 0.1214 - val_accuracy: 0.9670\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 0.0946 - accuracy: 0.9733 - val_loss: 0.1194 - val_accuracy: 0.9668\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 11s 226us/step - loss: 0.0930 - accuracy: 0.9740 - val_loss: 0.1183 - val_accuracy: 0.9672\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 12s 253us/step - loss: 0.0915 - accuracy: 0.9743 - val_loss: 0.1174 - val_accuracy: 0.9680\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 11s 226us/step - loss: 0.0900 - accuracy: 0.9750 - val_loss: 0.1165 - val_accuracy: 0.9682\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 11s 227us/step - loss: 0.0886 - accuracy: 0.9752 - val_loss: 0.1175 - val_accuracy: 0.9672\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 10s 210us/step - loss: 0.0872 - accuracy: 0.9758 - val_loss: 0.1160 - val_accuracy: 0.9676\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 12s 254us/step - loss: 0.0858 - accuracy: 0.9762 - val_loss: 0.1143 - val_accuracy: 0.9687\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 12s 247us/step - loss: 0.0846 - accuracy: 0.9764 - val_loss: 0.1140 - val_accuracy: 0.9687\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.0832 - accuracy: 0.9767 - val_loss: 0.1139 - val_accuracy: 0.9682\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 12s 260us/step - loss: 0.0820 - accuracy: 0.9772 - val_loss: 0.1124 - val_accuracy: 0.9695\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 13s 268us/step - loss: 0.0806 - accuracy: 0.9774 - val_loss: 0.1122 - val_accuracy: 0.9687\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 13s 277us/step - loss: 0.0794 - accuracy: 0.9778 - val_loss: 0.1110 - val_accuracy: 0.9690\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 14s 284us/step - loss: 0.0782 - accuracy: 0.9782 - val_loss: 0.1108 - val_accuracy: 0.9695\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 13s 271us/step - loss: 0.0771 - accuracy: 0.9788 - val_loss: 0.1092 - val_accuracy: 0.9697\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0761 - accuracy: 0.9793 - val_loss: 0.1093 - val_accuracy: 0.9704\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.0748 - accuracy: 0.9792 - val_loss: 0.1085 - val_accuracy: 0.9704\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 11s 234us/step - loss: 0.0738 - accuracy: 0.9796 - val_loss: 0.1089 - val_accuracy: 0.9697\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.0726 - accuracy: 0.9802 - val_loss: 0.1077 - val_accuracy: 0.9702\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.0716 - accuracy: 0.9806 - val_loss: 0.1066 - val_accuracy: 0.9703\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 9s 186us/step - loss: 0.0706 - accuracy: 0.9810 - val_loss: 0.1060 - val_accuracy: 0.9708\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 12s 260us/step - loss: 0.0696 - accuracy: 0.9813 - val_loss: 0.1060 - val_accuracy: 0.9707\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 9s 197us/step - loss: 0.0685 - accuracy: 0.9813 - val_loss: 0.1047 - val_accuracy: 0.9706\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.0677 - accuracy: 0.9818 - val_loss: 0.1046 - val_accuracy: 0.9711\n",
      "Epoch 38/50\n",
      " 1920/48000 [>.............................] - ETA: 14s - loss: 0.0821 - accuracy: 0.9750 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.118482). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 11s 239us/step - loss: 0.0667 - accuracy: 0.9820 - val_loss: 0.1037 - val_accuracy: 0.9713\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 11s 230us/step - loss: 0.0658 - accuracy: 0.9825 - val_loss: 0.1040 - val_accuracy: 0.9712\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.0648 - accuracy: 0.9829 - val_loss: 0.1036 - val_accuracy: 0.9708\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 12s 260us/step - loss: 0.0639 - accuracy: 0.9831 - val_loss: 0.1024 - val_accuracy: 0.9712\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 11s 223us/step - loss: 0.0630 - accuracy: 0.9832 - val_loss: 0.1018 - val_accuracy: 0.9712\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 13s 272us/step - loss: 0.0622 - accuracy: 0.9837 - val_loss: 0.1018 - val_accuracy: 0.9716\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 8s 166us/step - loss: 0.0613 - accuracy: 0.9841 - val_loss: 0.1011 - val_accuracy: 0.9718\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0605 - accuracy: 0.9842 - val_loss: 0.1010 - val_accuracy: 0.9720\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0595 - accuracy: 0.9847 - val_loss: 0.1008 - val_accuracy: 0.9714\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0590 - accuracy: 0.9848 - val_loss: 0.0996 - val_accuracy: 0.9713\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0582 - accuracy: 0.9848 - val_loss: 0.0995 - val_accuracy: 0.9718\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0573 - accuracy: 0.9853 - val_loss: 0.0993 - val_accuracy: 0.9719\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.0565 - accuracy: 0.9857 - val_loss: 0.0995 - val_accuracy: 0.9718\n",
      "Test score: 0.08979356143884361\n",
      "Test accuracy: 0.9715999960899353\n"
     ]
    }
   ],
   "source": [
    "# Aaron Cimielli - March 11 2023 \n",
    "# Number of epochs\n",
    "# for this experiment I modified the number of epochs.\n",
    "# Specifically, I decided to use 3 different values, \"10, 30, 50\". \n",
    "# Here I am using two hidden layers. \n",
    "\n",
    "# Examples were found in chapter 1 and 2 from Deep learing with Keras\n",
    "# and from https://keras.io/guides/training_with_built_in_methods ( site found in chapter 1 )\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# 2 hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Experiment 3: Change number of epochs\n",
    "nb_epochs = [10, 30, 50]\n",
    "\n",
    "for nb_epoch in nb_epochs:\n",
    "    print(f\"\\nExperiment with {nb_epoch} epochs\")\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        batch_size=BATCH_SIZE, epochs=nb_epoch,\n",
    "                        verbose=1, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1 {\n",
    "        font-family: \"Times New Roman\", Times, serif;\n",
    "        font-size: 30px;\n",
    "        font-weight: bold;\n",
    "        text-align: center;\n",
    "        margin-top: 50px;\n",
    "        margin-bottom: 50px;\n",
    "        border-bottom: 3px solid #000;\n",
    "        text-transform: uppercase;\n",
    "    }\n",
    "\n",
    "    p {\n",
    "        font-family: \"Times New Roman\", Times, serif;\n",
    "        font-size: 20px;\n",
    "        text-align: justify;}\n",
    "    i {\n",
    "        font-family: \"Times New Roman\", Times, serif;\n",
    "        font-size: 12px;\n",
    "    }\n",
    "</style>\n",
    "        \n",
    "<h1>Exploring the Impact of Neural Network Parameters on Accuracy Rates</h1>\n",
    "\n",
    "<p>In this project, I investigated the impact of various parameters on the accuracy rates of a neural network. The MNIST dataset, which is a collection of handwritten digits, was used to train and evaluate the neural network. I focused on three main parameters: the number of hidden layers, batch size, and number of epochs.</p>\n",
    "\n",
    "<h1>Experiment 1: Number of Hidden Layers</h1>\n",
    "\n",
    "<p style=\"padding-bottom: 10px\">The number of hidden layers in a neural network is a crucial parameter that determines its complexity. I modified this parameter by testing three different values: <b>1, 5, and 20</b>, and evaluated the accuracy of each model. The table below presents the accuracy rates for each neural network model on the train, validation, and test data:</p> \n",
    "\n",
    "<i>The table below summarizes the results of the experiment.</i>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Number of Epochs: | 20  | Size of Datasets: | 48000 |Batch Size :|128||\n",
    "|-------------------|-----|-------------------|-------|-----|-----|-----|\n",
    "Hidden Layers|Loss   |Accuracy |Validation Loss |Validation Accuracy |Test Score |Test Accuracy \n",
    "1            |1.7804 |0.2443   |1.7743          |0.2388              |1.7786     |0.2407\n",
    "5            |0.6272 |0.8281   |0.6046          |0.8317              |0.6048     |0.8365\n",
    "20           |0.9737 |0.6314   |0.9557          |0.6442              |0.961      |0.6309\n",
    "\n",
    "</div>\n",
    "\n",
    "<p style=\"padding-top: 20px\">As shown in the table, the neural network with <b>5 hidden layers</b> achieved the highest accuracy rates on the validation and test sets, while the neural network with <b>1 hidden layer</b> performed the worst. Interestingly, the neural network with <b>20 hidden layers</b> did not perform as well as the neural network with <b>5 hidden layers</b> in terms of accuracy rate. Although, it did have the lowest validation loss and test score among all the models. This suggests that increasing the number of hidden layers beyond a certain point may not necessarily lead to better performance, and may even result in overfitting.</p>\n",
    "\n",
    "\n",
    "<p>This experiment highlights the importance of choosing an appropriate number of hidden layers to achieve the best performance in a neural network. It is also important to consider the trade-off between accuracy and overfitting when selecting the number of hidden layers.</p>\n",
    "\n",
    "\n",
    "<h1>Experiment 2: Exploring the Impact of Batch Size on Accuracy Rates</h1>\n",
    "\n",
    "<p style=\"padding-bottom: 20px\">In this experiment, I aimed to explore the impact of batch size on the accuracy rates of a neural network. To do this, I trained and evaluated neural networks with different batch sizes on a dataset of handwritten digits (MNIST).</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Number of Epochs:| 20 | Size of Datasets:| 48000 | Hidden Layers:|128||\n",
    "|---------|-----------|---------------|---------------|--------------|-----|-----|\n",
    "Batch Size |Loss   |Accuracy |Validation Loss |Validation Accuracy |Test Score |Test Accuracy \n",
    "32         |0.0731 |0.9799   |0.2614          |0.9266              |1.7786     |0.2407\n",
    "64         |0.0411 |0.9896   |0.0934          |0.9726              |0.0839     |0.9739\n",
    "256        |0.0345 |0.9921   |0.0916          |0.9720              |0.0809     |0.9751\n",
    "\n",
    "</div>\n",
    "\n",
    "<p style=\"padding-top: 20px\">As shown in the table, increasing the batch size from <b>32 to 64</b> improved the accuracy rates on the validation and test sets, as well as reducing the loss on the test set. However, further increasing the batch size from <b>64 to 256</b> did not result in significant improvements in accuracy rates, and only resulted in a slight reduction in loss on the test set. This suggests that using a batch size that is too large may not necessarily lead to better performance.</p>\n",
    "\n",
    "<p>This experiment highlights the importance of selecting an appropriate batch size for training a neural network, and that the optimal batch size may depend on the specific dataset and model architecture being used.</p>\n",
    "\n",
    "<h1>Experiment 3: Exploring the Impact of Number of Epochs on Accuracy Rates</h1>\n",
    "\n",
    "<p style=\"padding-bottom: 20px\">In this experiment, I aimed to explore the impact of the number of epochs on the accuracy rates of a neural network. To do this, I trained and evaluated neural networks with different numbers of epochs on a dataset of handwritten digits (MNIST).</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Size of Datasets:| 48000 | Hidden Layers:|128|Batch Size: |128||\n",
    "|---------|-----------|---------------|---------------|--------------|-----|-----|\n",
    "Number of Epochs |Loss   |Accuracy |Validation Loss |Validation Accuracy |Test Score |Test Accuracy \n",
    "10               |0.2710 |0.9228   |0.2534          |0.9284              |0.2528     |0.9276\n",
    "30               |0.1232 |0.9655   |0.1378          |0.9635              |0.1311     |0.9613\n",
    "50               |0.0565 |0.9857   |0.0995          |0.9718              |0.0897     |0.9715\n",
    "\n",
    "</div>\n",
    "\n",
    "<p style=\"padding-top:20px\">As shown in the table, increasing the number of epochs from <b>10 to 30</b> resulted in significant improvements in accuracy rates on the validation and test sets, as well as reducing the loss on the test set. However, further increasing the number of epochs from <b>30 to 50</b> did not result in significant improvements in accuracy rates, and only resulted in a slight reduction in loss on the test set. This suggests that using too many epochs may lead to overfitting and may not necessarily lead to better performance.</p>\n",
    "\n",
    "<p>This experiment highlights the importance of selecting an appropriate number of epochs for training a neural network, and that the optimal number of epochs may depend on the specific dataset and model architecture being used.</p>\n",
    "\n",
    "<h1>Summary of Results Across Experiments</h1>\n",
    "<p>In my exploration of neural networks, I conducted several experiments to study the impact of different parameters on accuracy rates. Through my experiments, I found that selecting appropriate values for these parameters is essential in achieving optimal performance. Specifically, I discovered that increasing the number of hidden layers can improve accuracy rates up to a certain point, but beyond that point, further increases may lead to overfitting. Additionally, I found that selecting an appropriate batch size is crucial for achieving the best results, as using a batch size that is too large may not necessarily lead to better performance. Finally, I found that increasing the number of epochs can improve accuracy rates, but using too many epochs may lead to overfitting and reduced performance.</p>\n",
    "<p>These experiments highlight the importance of carefully selecting values for various neural network parameters to achieve the best possible performance. I found that considering both accuracy and overfitting when selecting the number of hidden layers, batch size, and number of epochs is crucial. These findings can be useful for designing and optimizing neural networks for a range of applications.</p>\n",
    "\n",
    "\n",
    "\n",
    "<h1>References</h1>\n",
    "\n",
    "- Gulli, A., & Pal, S. (2017). *Deep learning with keras: Get to grips with the basics of keras to implement fast and efficient deep-learning models*. Packt Publishing, Limited.\n",
    "- Keras. (n.d.). Guides. Retrieved March 6, 2023, from https://keras.io/guides/\n",
    "- W3Schools. (n.d.). Neural Networks. Retrieved March 06, 2023, from https://www.w3schools.com/ai/ai_neural_networks.asp\n",
    "racy of the neural network.\n",
    "\n",
    "Overall, we can see that different parameters have different effects on the accuracy of the neural network. Increasing the number of hidden layers can make the neural network too complex and cause it to overfit the data, while increasing the batch size and number of epochs can improve the accuracy of the neural network up to a certain point. It is important to carefully choose these parameters to optimize the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
